{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSX Proof of Concept \n",
    "*by Graham Lim, Data Science Consultant for FSX*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be demonstrating our **preliminary Proof of Concept (\"POC\")** for our **FSX Risk Decisioning Platform (the \"Platform\")**.\n",
    "This will be written in `python`, with explanatory notes given to explain each step of our proprietary process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Summary\n",
    "Conceptually, the Platform is a supervised machine learning (ML) media sentiment classifier that mines news articles automatically that deal with the subject of food supply risk, and predictively classifies them according to their risk labelling. This will utilize statistical ML models like those from `scikit-learn`, as well as Deep Learning/ Neural Networks, such as Google's open-source `BERT` Transformers.\n",
    "\n",
    "Based on that risk labelling, an automated text report with visualizations is generated with insights. Initially, this will employ standard template-based code. Future versions will incorporate unsupervised text generation algorithms (e.g. `GPT2`) and/or commercial text-generation software (e.g. `Arria NLG`).\n",
    "\n",
    "**Disclaimer**: Our POC has been designed for the sake of demonstrating capability and conceptual proof only, and is not strictly optimized for typical Machine Learning performance metrics (e.g. `accuracy`, `precision`, `recall`, `f1 score` etc.) due to the small sample dataset used here.\n",
    "\n",
    "## POC Contents:\n",
    "1. **Webscraping**: prototype code that scrapes a sample of articles from the web for news about food supply risk using `Selenium` and `BeautifulSoup`, which are open-source libraries that help us scrape. Future versions will scrape the web as comprehensively as possible, beyond the sample. Future versions will have a far larger and statistically significant dataset that is scraped.\n",
    "\n",
    "\n",
    "2. **Cleaning and Labelling**: prototype code that transforms sample scraped news data into a Dataframe in .csv format, that is machine readable. We use the popular `Pandas` library for this. \n",
    "\n",
    "    We initially manually label the risk profiles of each news article in our sample data, for the sake of being able to train our prototype model. Note again that future versions will have a far larger and statistically significant dataset.\n",
    "\n",
    "\n",
    "3. **Preprocessing and Modelling**: The text that we scraped has to be pre-processed into a machine-readable format e.g. tokenization, stop-words removal, and lemmatization. We then run models from `scikit-learn` or a custom Neural Network e.g. BERT on this machine-readable data in order to predictively classify news text as falling into a particular risk category e.g. `high`, `medium`, `low`, or `neutral`. \n",
    "\n",
    "    For the sake of simplicity in this PoC we will use `risky`, `neutral` and `stable` as our preliminary labels.\n",
    "    \n",
    "\n",
    "4. **Report Generator**: our report-generation algorithm is intended to provide actionable insights based on that risk assessment. In our prototype here, the report-generator will provide the following information:\n",
    "\n",
    "    * Reiterate the risk label\n",
    "    \n",
    "    * Describe which keywords in the article triggered our label to be e.g. risky\n",
    "    \n",
    "    * Describe which entities are being discussed in the article\n",
    "    \n",
    "    * Based on the risk label, the algorithm **recommends** via text that other suppliers of the food item in question be reviewed.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to run the following pip install commands in terminal or cmd line:\n",
    "\n",
    "* `pip install bs4` (for BeautifulSoup)\n",
    "* `pip install selenium` (for Selenium)\n",
    "* `pip install webdriver-manager` (for the automated Selenium web driver to work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Python DS imports:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set column size to be larger\n",
    "pd.set_option(\"display.max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to use `Selenium` because of the fact that all the articles won't show up on one webpage and it's easier to scrape them this way./\n",
    "\n",
    "Hence, we will import `Selenium` and the related `WebDriver Manager` tool to run a Chrome instance within Selenium that will scrape our sample food supply news data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selenium and WebDriver Manager imports:\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our sample news feed, we are taking all articles tagged with the keyword `Rice` from `www.foodnavigator-asia.com`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.foodnavigator-asia.com/Trends/Supply-chain?page=\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the scraping code for this particular website. Future versions of this code will utilize a larger dataset aggregated from other news sites.\n",
    "For the initial phase of development, this would essentially involve a large-scale media analysis undertaking to manually label data first in order to train our supervised labelling model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code will scrape data from the URL in question\n",
    "# driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "def getPages(url):\n",
    "    driver = webdriver.Chrome(\"C:/Users/User/Desktop/FYP/chromedriver.exe\") \n",
    "    driver.get(url)    \n",
    "    numPages = driver.find_element_by_xpath(\"/html/body/div[2]/div/main/div[1]/div/ul/li[7]/a\").text\n",
    "    driver.close()\n",
    "    return numPages\n",
    "\n",
    "\n",
    "\n",
    "intro_list = []\n",
    "title_list = []\n",
    "date_list = []\n",
    "post_url_list = []\n",
    "    \n",
    "def getArticles(target_url):     \n",
    "    numPages = int(getPages(target_url))\n",
    "    numPages += 1\n",
    "\n",
    "    for i in range(1,numPages):\n",
    "        driver = webdriver.Chrome(\"C:/Users/User/Desktop/FYP/chromedriver.exe\")\n",
    "        url = target_url + str(i)\n",
    "        driver.get(url)\n",
    "#         time.sleep(15)\n",
    "#         driver_body = driver.find_element_by_tag_name('body')\n",
    "        driver_body = WebDriverWait(driver,20).until(EC.visibility_of_all_elements_located((By.TAG_NAME,\"body\")))\n",
    "\n",
    "        html = \"\"\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        intro_elems = driver.find_elements_by_class_name(\"Teaser-intro\")\n",
    "\n",
    "        for intro in intro_elems:\n",
    "            intro_list.append(intro.text)\n",
    "\n",
    "        title_elems = driver.find_elements_by_class_name(\"Teaser-title\")\n",
    "\n",
    "        for title in title_elems:\n",
    "            title_list.append(title.text)\n",
    "\n",
    "        date_elems = driver.find_elements_by_class_name(\"Teaser-date\")\n",
    "        for date in date_elems:\n",
    "            date_list.append(date.text)\n",
    "\n",
    "        linksdiv = soup.find_all('h3', {'class': 'Teaser-title'})\n",
    "        for linkdiv in linksdiv:\n",
    "            post_url_list.append('www.foodnavigator-asia.com'+(linkdiv.find('a')['href']))\n",
    "\n",
    "        driver.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "getArticles(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2889\n",
      "2889\n",
      "2889\n",
      "2889\n"
     ]
    }
   ],
   "source": [
    "#checking scraped length\n",
    "print (len(intro_list))\n",
    "print (len(title_list))\n",
    "print (len(date_list))\n",
    "print (len(post_url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2889, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We then write a simple function to convert and label these lists as DataFrames in pandas, and tells us what the `shape` of the dataframe is:\n",
    "\n",
    "df = pd.DataFrame({'date': date_list,\n",
    "                   'title': title_list,\n",
    "                   'intro': intro_list,\n",
    "                   'url': post_url_list\n",
    "                  })\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the result of our initially scraped news data looks like that our algorithm uses. In future, we will not only use the `title` and `intro` text of news articles, but the full text of the actual news article for better information capturing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>intro</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>17-Mar-2004</td>\n",
       "      <td>China-Vietnam food trade boost</td>\n",
       "      <td>Trade in food and beverage products is expected to grow\\nsignificantly on the back of an agreement made between China and\\nVietnam which will end tariffs on nearly 500 seafood and\\nagricultural products during the course of the next...</td>\n",
       "      <td>www.foodnavigator-asia.com/Article/2004/03/17/China-Vietnam-food-trade-boost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date                           title  \\\n",
       "2888  17-Mar-2004  China-Vietnam food trade boost   \n",
       "\n",
       "                                                                                                                                                                                                                                            intro  \\\n",
       "2888  Trade in food and beverage products is expected to grow\\nsignificantly on the back of an agreement made between China and\\nVietnam which will end tariffs on nearly 500 seafood and\\nagricultural products during the course of the next...   \n",
       "\n",
       "                                                                               url  \n",
       "2888  www.foodnavigator-asia.com/Article/2004/03/17/China-Vietnam-food-trade-boost  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.head()\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"csv_data/food_navigator.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- break ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning and Labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data has been scraped, we have to re-format it and label the risk ratings of each our sample news feed articles in a way that our machine learning models will understand. For example, we'll initially have to combine the `title text` and `intro text` of each news article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/df_raw.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#checking for null values\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine title and intro text in one\n",
    "\n",
    "df[\"combined\"] = df[\"title\"] + \". \" + df[\"intro\"]\n",
    "\n",
    "df[\"combined\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create a default risk rating of neutral first for all news articles\n",
    "df[\"risk_rating\"]=\"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually annotate as risky, neutral or stabilizing to Asia rice food supply\n",
    "\n",
    "#going through 10 rows at a time to annotate\n",
    "\n",
    "#all news is neutral unless otherwise annotated as risky or stabilizing\n",
    "\n",
    "\n",
    "risky_rows = [2, 3, 7, 13, 17, 18, 20, 37, 43, 49, 53, 54, 58, 68, 71]\n",
    "stabilizing_rows = [6, 8, 9, 10, 11, 12, 14, 21, 24, 25, 27, 29, 30, 32, 42, \n",
    "                    46, 47, 48, 51, 55, 57, 61, 64, 65, 70, 73, 74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(risky_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stabilizing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"combined\"]][60:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we replace our placeholder \"neutral\" rating in our manual annotations of risky rows \n",
    "for i in risky_rows:\n",
    "    df.at[i, \"risk_rating\"] = \"risky\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stabilizing_rows:\n",
    "    df.at[i, \"risk_rating\"] = \"stable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#risky ratings are in, looks ok\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"risk_rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_ratings = df[\"combined\"][df[\"risk_rating\"]==\"stable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"risk_rating\"]==\"stable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[stabilizing_rows, \"risk_rating\"] = \"stable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now label the risk rating in this intial `training dataset` so that our machine learning models will be able to learn what text ought to be classified as what rating in the future automatically. \n",
    "\n",
    "Again, for simplicity's sake, we use simpler labels like `risky`, `neutral`, and `stable`; future version swill have more nuanced risk labelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df[\"risk_rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/df_ratings_raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3A. Pre-processing our Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/df_ratings_raw.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will require a number of pre-processing tools that we import into our python scripts. The purpose of these tools is to further convert the dataset into a machine-readable format, by splitting our data into a training, validation and test set, as well as utilizing Natural Language Processing to convert our text into ML-compatible Word Vectors.\n",
    "\n",
    "The tools we use are:\n",
    "* Preprocessing tools from `scikit-learn`, which is a commercially-used open-source Python-based machine-learning library (https://scikit-learn.org/stable/), and\n",
    "* Preprocessing tools from `spaCy`, which is an \"Industrial-Strength\"open-source Python-based Natural Language Processing library (https://spacy.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn modelling tools\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#for multilabel classification and oversampling\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "#for PCA - Dimensionality Reduction\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "#for word vectorization\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#our dummy baseline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "#advanced supervised classification models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#classification metrics imports \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, hinge_loss\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-process text for EDA and later modelling too using spacy and string libraries\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English # updated\n",
    "\n",
    "import re\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "import spacy.cli\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc\n",
    "from spacy import displacy\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've imported our tools, we write code using `spaCy` to transform words into numerical arrays known as Word Vectors. This involves a process coded out below known as `tokenization`.\n",
    "\n",
    "We also convert all of the words in our dataset into their lemmatized form (base word-format e.g. the base format of \"disappointed\" is \"disappoint\", as well remove punctuation, stop-words, numbers. \n",
    "\n",
    "Again, all of these steps are necessary to allow our models to \"read\" the language and predict what risk scoring should be assigned to any given news article. \n",
    "\n",
    "Here is what the pre-processing code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords removal\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    \n",
    "    #remove non-words\n",
    "    tokens = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "    tokens = re.sub(\"[0-9]+\", \"\", tokens)\n",
    "    \n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    tokens = parser(tokens)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    tokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    tokens = [ word for word in tokens if word not in stop_words and word not in punctuations]\n",
    "    \n",
    "    # return preprocessed list of tokens\n",
    "    return(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bag_cleaned_text\"]=df[\"combined\"].apply(spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithm will also assign a numerical risk score based on the risk categories with this dictionary of values for multiclass classification to work in sklearn. The purpose of this is for future risk-weighted scoring for our report-writing algorithm, which is explained later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"risk_rating_numerical\"] = df[\"risk_rating\"].copy()\n",
    "print(df[\"risk_rating_numerical\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical risk rating score impact\n",
    "df[\"risk_rating_numerical\"].replace({'neutral':0, \n",
    "                           'stable':1, \n",
    "                           'risky':-1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final part of our pre-processing, we now write code to actually transform our text into individual word and 2-word phrase (bigram) tokens, which will attach numerical scores that are weighted tf: term frequency(count of the words present in document from its own vocabulary), idf: inverse document frequency(importance of the word to each document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the features (tf-idf weights) for the processed text\n",
    "\n",
    "texts = df['bag_cleaned_text'].astype('str')\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), \n",
    "                                   min_df = 2, \n",
    "                                   max_df = .95)\n",
    "\n",
    "X = tfidf_vectorizer.fit_transform(texts) #features\n",
    "y = df['risk_rating_numerical'].values #target\n",
    "\n",
    "print (X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3B. Modelling: Multilabel Text Classification using `scikit-learn`\n",
    "Here, we're going to train our data using the following `scikit-learn` machine learning models: \n",
    "* Random Forest, \n",
    "* XGBoost, \n",
    "* SVC, and \n",
    "* K-Nearest Neighbours.\n",
    "\n",
    "We'll split the data into a train, validation and test set. We'll only run our models on the validation set to understand generalization of each model, which we compare against sklearn's own DummyClassifier as our baseline model.\n",
    "\n",
    "The purpose of all this is to evaluate whether these conventional `sci-kit learn` machine learning models will perform better at predicting the right risk label for any given piece of news. We also compare this against an NLP-focused Deep Neural Network known as `BERT`, which is Google's own open-source Deep Learning model: (https://en.wikipedia.org/wiki/BERT_(language_model))\n",
    "\n",
    "**Once the first phase of our training data has been compiled and labelled, our risk decisioning engine will constantly be evaluated for performance. We will, at all times, be asking ourselves - are the models we are using good at predicting risk correctly?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#General model evaluation using default parameters on Validation Set\n",
    "\n",
    "#Creating a dict of the models\n",
    "model_dict = {'Random Forest': RandomForestClassifier(random_state=3),\n",
    "              'XGBoost': XGBClassifier(random_state=3),\n",
    "              'SVC':SVC(),\n",
    "              'K Nearest Neighbor': KNeighborsClassifier(),\n",
    "              'Dummy Baseline': DummyClassifier()} \n",
    "\n",
    "\n",
    "#Train test split with stratified sampling for evaluation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size = .3, \n",
    "                                                    shuffle = True, \n",
    "                                                    stratify = y, \n",
    "                                                    random_state = 3)\n",
    "\n",
    "\n",
    "print(\"Multiclass Clause Classification - Validation Scores\")\n",
    "\n",
    "#Function to get the scores for each model in a datframe\n",
    "def model_score_df(model_dict):   \n",
    "    \n",
    "    model_name, ac_score_list, p_score_list, r_score_list, f1_score_list, auc_list = [], [], [], [], [], []\n",
    "    \n",
    "    for k,v in model_dict.items():   \n",
    "        model_name.append(k)\n",
    "        v.fit(X_train, y_train)\n",
    "        y_pred = v.predict(X_val)\n",
    "        ac_score_list.append(accuracy_score(y_val, y_pred))\n",
    "        p_score_list.append(precision_score(y_val, y_pred, average='micro'))\n",
    "        r_score_list.append(recall_score(y_val, y_pred, average='micro'))\n",
    "        f1_score_list.append(f1_score(y_val, y_pred, average='micro'))\n",
    "\n",
    "        model_comparison_df = pd.DataFrame([model_name, ac_score_list, p_score_list, r_score_list, f1_score_list]).T\n",
    "        model_comparison_df.columns = ['model_name', 'accuracy_score', 'precision_score', 'recall_score', 'f1_score']\n",
    "        model_comparison_df = model_comparison_df.sort_values(by='f1_score', ascending=False)\n",
    "    return model_comparison_df\n",
    "\n",
    "model_score_df(model_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3C. BERT Modelling\n",
    "We also will utilize a Deep Learning Neural Network known as `BERT`, mentioned earlier. \n",
    "\n",
    "Kaushal Trivedi defines `BERT` as a \"multilingual transformer based model that has achieved state-of-the-art results on various NLP tasks. BERT is a bidirectional model that is based on the transformer architecture, it replaces the sequential nature of RNN (LSTM & GRU) with a much faster Attention-based approach. The model is also pre-trained on two unsupervised tasks, masked language modeling and next sentence prediction. This allows us to use a pre-trained BERT model by fine-tuning the same on downstream specific tasks such as sentiment classification, intent detection, question-answering and more.\" (Source: https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d)\n",
    "\n",
    "For our purposes, it is another model on top of `scikit-learn`'s ML models that can help us predict the correct risk label for a given set of news. The code below allows us to use a wrapper called `ktrain` that trains a BERT model on our news risk classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please install ktrain on Google Colab: `pip install ktrain`\n",
    "\n",
    "#BERT ktrain wrapper imports\n",
    "import ktrain\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"combined\"]\n",
    "y = df[\"risk_rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#I have to change our train test split objects into a list of strings.\n",
    "X_train = X_train.values.tolist()\n",
    "X_test = X_test.values.tolist()\n",
    "\n",
    "y_train = y_train.values.tolist()\n",
    "y_test = y_test.values.tolist()\n",
    "\n",
    "print(\"classes to predict\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recalling our Y label dictionary\n",
    "encoding = {'neutral':0, \n",
    "            'stable':1, \n",
    "            'risky':-1}\n",
    "\n",
    "# Integer values for each class\n",
    "y_train = [encoding[x] for x in y_train]\n",
    "y_test = [encoding[x] for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we now run BERT-specific preprocessing:\n",
    "class_names = ['neutral','stable','risky']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,  y_train), (x_test, y_test), preproc = text.texts_from_array(x_train=X_train, y_train=y_train,\n",
    "                                                                       x_test=X_test, y_test=y_test,\n",
    "                                                                       class_names=class_names,\n",
    "                                                                       preprocess_mode='bert',\n",
    "                                                                       maxlen=512, \n",
    "                                                                       max_features=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = text.text_classifier('bert', train_data=(x_train, y_train), preproc=preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ktrain.get_learner(model, train_data=(x_train, y_train), \n",
    "                             val_data=(x_test, y_test),\n",
    "                             batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_onecycle(2e-5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out a performance report\n",
    "learner.validate(val_data=(x_test, y_test), class_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that once again neither `BERT` nor the `scikit-learn` models in this POC has a large enough dataset that is optimized for ML classification metrics `accuracy/precision/recall/f1 score` in the prototype; the production model will still involve a team of media analysts supervising and correcting the initial risk labelling so as to \"teach\" our AI models what it needs to predict. The purpose of showing the programming and logic here for our POC here is a showcase of capability that we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What the output of our Classification/Labelling Models looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modelling phase explained earlier will **generate a .csv file** that will look like the DataFrame below. Since we are still developing the models, we have created a preview of what 2 AI-classified/labelled news articles would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report = pd.read_csv(\"../data/df_report_demo.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model will predict the `risk rating (both verbally and numerically)`, the `main_entities` discussed, the `commodity` at risk, and whether or not the risk concerned is on the supply-side or demand side (`is_supply_shock`).\n",
    "\n",
    "This data, as well as the risk-scoring ramifications based on this, will then be procedurally generated by our text report generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Report Generating Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our risk-labelling AI model, we can then create a separate algorithm that generates a risk report on any given news article based on the risk label predicted using our chosen AI model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report will contain the following key components:\n",
    "* NLP Multi-label: What is the `commodity` in question?\n",
    "* NLP Multi-label: Which `country` is mainly in question? What other `entities` are talked about here?\n",
    "* NLP Binary-class: Is the shock `demand-based` or `supply-based` shock?\n",
    "* NLP Multi-label: Based on the risk label, What is the `new risk score` of country or countries in question for that commodity? Do we apply `+1, 0 -1`?\n",
    "    * Update all countries's risk ratings accordingly\n",
    "* If the risk score of the country in question is lowered, can we recommend other countries with a better risk score for supply of this commodity?\n",
    "\n",
    "* What does the `AI-summarized text of the entire article` look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report can be generated as a table with an accompanying visualization uses spaCy's `Named Entity Recognition`, and our production model will be able to generate an individual report for each news article that relates to food supply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk Matrices\n",
    "\n",
    "For the report generator to readjust risk-weighted scores from food suppliers, we will require not only the model-generated .csv file, but we'll also have to create professionally curated `risk matrices` of commodity suppliers worldwide. \n",
    "\n",
    "Similar to credit scoring, this will be based on proprietary risk-weighting methods pioneered by FSX.ai's supply chain and risk experts. A score is assigned to individual country suppliers, with the idea being that the higher the number, the more secure that supplier's capacity to supply a particular commodity is, and vice-versa.\n",
    "\n",
    "For example, because India is now the largest supplier of rice worldwide (~32.5% of worldwide supply), their supply risk score is higher than say Spain, which only accounts for 0.9% of worldwide rice supply. \n",
    "\n",
    "This also therefore means that negative news (resulting in a risk rating of `-1`) is more impactful on the new risk score for India than it is to Spain, and it suffers a `risk penalty multiplier` of `3.25`. Therefore, negative news about India's rice supplying capacity will result in its score being subtracted by `-3.25`, whereas Spain's penalty will only be `-0.09`.\n",
    "\n",
    "For the sake of our demo, we have employed this simplistic calculation. We also assigned risk scores and penalties arbitrarily for ease of demonstration. Again, our production model will incorporate the fully articulated risk-scoring calculations based on curated weightages divined from our supply chain and risk experts. \n",
    "\n",
    "Accordingly, we create here a `sample risk score matrix of rice suppliers` worldwide for demo purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example matrix based on http://www.worldstopexports.com/rice-exports-country/\n",
    "supplier_data = [['india', 32.5], ['thailand', 19.2], ['united_states', 8.6], ['vietnam', 6.6], \n",
    "                 ['pakistan', 5.6], ['china', 4.8], ['italy', 2.9], \n",
    "                 ['myanmar_burma', 2.6], ['cambodia', 2], ['uruguay', 1.7], \n",
    "                 ['brazil', 1.7], ['netherlands', 1.7], ['belgium', 1.2],\n",
    "                 ['paraguay', 1], ['spain', 0.9]]\n",
    "        \n",
    "df_rice_supplier_matrix = pd.DataFrame(supplier_data, columns = [\"supplier_country\", \"current_risk_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying demo risk penalty multiplier as a new column in the matrix dataframe\n",
    "df_rice_supplier_matrix['risk_penalty_multiplier'] = df_rice_supplier_matrix['current_risk_score'].map(lambda x: x / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rice_supplier_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_countries = [state for state in df_rice_supplier_matrix[\"supplier_country\"] if state!=\"thailand\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rice_supplier_matrix[df_rice_supplier_matrix[\"supplier_country\"] != \"thailand\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rice_supplier_matrix[(df_rice_supplier_matrix[\"supplier_country\"] != \"thailand\")\n",
    "                        & (df_rice_supplier_matrix[\"current_risk_score\"] >= 2.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backing up this matrix\n",
    "df_rice_supplier_matrix.to_csv(\"../data/df_supplier_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_prior_score = df_rice_supplier_matrix[\"current_risk_score\"][df_rice_supplier_matrix[\"supplier_country\"] == \"thailand\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_prior_score.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [3]: sub_df\n",
    "Out[3]:\n",
    "          A         B\n",
    "2 -0.133653 -0.030854\n",
    "\n",
    "In [4]: sub_df.iloc[0]\n",
    "Out[4]:\n",
    "A   -0.133653\n",
    "B   -0.030854\n",
    "Name: 2, dtype: float64\n",
    "\n",
    "In [5]: sub_df.iloc[0]['A']\n",
    "Out[5]: -0.13365288513107493"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype Code for How The Report Generator Works ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write a function that transforms this data into a text report with entity visualization using `spaCy's NER`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_0 = df_report.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_0[\"combined\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(row_0[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_writer(row):\n",
    "\n",
    "    #assign values into callable instances\n",
    "    date = row[\"date\"]\n",
    "    title = row[\"title\"]\n",
    "    url = row[\"url\"]\n",
    "    risk_rating = row[\"risk_rating\"]\n",
    "    risk_rating_numerical = row[\"risk_rating_numerical\"]\n",
    "    commodity = row[\"commodity\"]\n",
    "    main_entities = row[\"main_entities\"]\n",
    "    supply_shock = row[\"is_supply_shock\"]\n",
    "    minimum_score = 2\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    current_text = row['combined']\n",
    "    doc = nlp(current_text)\n",
    "    \n",
    "    print (f\"***************************START OF REPORT********************************\")\n",
    "    print (\"\\n\")\n",
    "\n",
    "    print (f\"This is an automatically generated report for:\")\n",
    "    print (f\"\\n'{title}', \\ndated {date}, \\nscraped from the url \\n{url}.\")\n",
    "    print (\"\\n\")\n",
    "    print (f\"Our FSX.ai Risk Model has identified that the article concerns risk towards the supply of {commodity.title()} by {main_entities.title()}.\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    #get previous score of this country's rice supply risk\n",
    "    state_prior_score = df_rice_supplier_matrix[\"current_risk_score\"][df_rice_supplier_matrix[\"supplier_country\"] == main_entities].iloc[0]    \n",
    "    \n",
    "    print (f\"{main_entities.title()}'s prior risk score for {commodity.title()} supply is {state_prior_score}\")\n",
    "    \n",
    "    #get previous score of this country's rice supply risk\n",
    "    state_penalty = df_rice_supplier_matrix[\"risk_penalty_multiplier\"][df_rice_supplier_matrix[\"supplier_country\"] == main_entities].iloc[0]    \n",
    "\n",
    "    print (f\"{main_entities.title()}'s risk penalty multiplier is {state_penalty}\")\n",
    "    \n",
    "    new_score = state_prior_score - (risk_rating_numerical*state_penalty)\n",
    "    \n",
    "    print (\"\\n\")\n",
    "    print (f\"{main_entities.title()}'s new risk score for {commodity.title()} supply is {new_score}\")\n",
    "        \n",
    "    print (\"\\n\")\n",
    "\n",
    "    print (f\"Currently, FSX.ai recommends choosing suppliers with a minimum risk score of {minimum_score}.\")\n",
    "    minimum_score_suppliers = df_rice_supplier_matrix[(df_rice_supplier_matrix[\"supplier_country\"] != main_entities)\n",
    "                                                      & (df_rice_supplier_matrix[\"current_risk_score\"] >= minimum_score)]\n",
    "    \n",
    "    print (f\"Accordingly, FSX.ai recommends reviewing the increase of imports from other suppliers of {commodity.title()}:\")\n",
    "    print (\"\\n\")\n",
    "    print (minimum_score_suppliers)\n",
    "    print (\"\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "    print (f\"FSX.ai's spaCy NER has also highlighted the following related entities in this article that should be reviewed \\nin relation to this news article:\")\n",
    "    print (\"\\n\")\n",
    "    print (displacy.render(doc, style='ent', jupyter=True))\n",
    "\n",
    "    print (f\"***************************END OF REPORT********************************\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # return df_rice_supplier_matrix\n",
    "#     print (f\"Accordingly, FSX.ai recommends reviewing the increase of imports from other suppliers of {commodity.title()}:\")\n",
    "#     print (\"\\n\")\n",
    "#     print (df_rice_supplier_matrix[df_rice_supplier_matrix[\"supplier_country\"] != main_entities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_writer(df_report.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype Article Summarizer \n",
    "Finally, the last component of our report addresses this query:\n",
    "* NLG Text Summarization: What is the `AI-summarized` version of the entire article?\n",
    "\n",
    "We have prototype code that summarizes entire news articles using `huggingface's T5 Transformer`: https://huggingface.co/transformers/model_doc/t5.html\n",
    "\n",
    "Let's first use a new example of an entire news article taken from `Channel News Asia`:https://www.channelnewsasia.com/news/asia/thailand-wetland-conservation-boon-reuang-special-economic-zone-12886250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\"\n",
    "DUTTA NAGAR, India: Until late March, Ashish Kumar was helping to make plastic boxes for \n",
    "Ferrero Rocher praline chocolates and the plastic spoons tucked inside Kinder Joy eggs to scoop out the \n",
    "milky sweet cream inside.\n",
    "\n",
    "With a diploma in plastic mould technology, the 20-year-old had a foot on his chosen career ladder. His younger brother Aditya chose law, but Ashish had his sights set on plastic.\n",
    "\n",
    "\"I want to start a business of my own,\" he said, explaining how he wants to recycle plastic to make day-to-day products at his own factory.\n",
    "\n",
    "India's coronavirus lockdown has thrown those plans into disarray. Educated but unemployed, Ashish Kumar is one of countless people across the globe whose social progress has been halted by the coronavirus that has infected more than 2 million people in India alone, and thrown the economy into reverse. With it, the aspirations of millions are fading.\n",
    "\n",
    "For years, people in rural India have been gaining prosperity and moving into what economists call a burgeoning middle class of consumers – those who earn more than US$10 a day, by some definitions. This group has been a keystone of plans for economic development in the world's second most populous country.\n",
    "\n",
    "In the COVID-19 pandemic, India's economy is forecast to shrink by 4.5 per cent this year, according to the International Monetary Fund. At least 400 million Indian workers are at risk of falling deeper into poverty, according to the International Labour Organization (ILO).\n",
    "\n",
    "Kumar is one of around 131,000 people whom local officials estimate returned from working around India to Gonda, the district in the northern state of Uttar Pradesh that he left last June.\n",
    "\n",
    "Nationwide, about 10 million people made long, hard journeys back to rural villages they had left. Some have gone back to the cities, but many of those who had been sending back funds are still stuck in the countryside.\n",
    "\n",
    "Working in a factory in Baramati in the western state of Maharashtra, Kumar was earning 13,000 rupees (US$173) every month, more than twice his father's pay from a job in a grain market near Kumar's home village in Uttar Pradesh, a sprawling agrarian state. Of that, the young man was sending home around 9,000 rupees every month, much of which was helping to fund his younger brother's studies.\n",
    "\n",
    "No longer. Once a provider for his family, now he has become a financial burden.\n",
    "\n",
    "Kumar whiles away his time back home in the village of Dutta Nagar, bantering with friends in the muddy courtyard – they jokingly call it their \"office\" – outside the ramshackle primary school where he studied. In Uttar Pradesh, around 60 million of the state's population of more than 200 million lives in poverty, according to the World Bank.\n",
    "\n",
    "He said he has applied for several jobs at plastic factories in western Gujarat state and other parts of northern India but has not found work.\n",
    "\n",
    "\"No matter what,\" he said, sitting near his parent's single-storey home, surrounded by jade green paddy fields. \"I need a job.\"\n",
    "\n",
    "PLASTIC FOR PRALINES\n",
    "\n",
    "As a schoolboy, Kumar was obsessed with plastics.\n",
    "\n",
    "A chance conversation with a cousin who had studied plastic engineering got him hooked, Kumar said, and he started researching. In Dutta Nagar, where there were no Internet connections, that often meant asking one of a handful of locals with a smartphone to Google the opportunities.\n",
    "\n",
    "Kumar's ambitions were a world removed from his father Ashok's early years. The 47-year-old, who assists with weighing and pricing grain harvests, remembers when the family had neither enough food, nor proper clothes.\n",
    "\n",
    "A slight man with a weather-beaten face, he never finished high school.\n",
    "\n",
    "\"I thought that the children shouldn't fall into our rut. They should be pushed ahead,\" he said.\n",
    "\n",
    "Kumar, who says he has never tasted a Ferrero Rocher praline, finished his diploma in Gujarat last June, and took the train to start work as a technician at an Italian-owned factory 1,500km away from home.\n",
    "\n",
    "The factory that employed him is run by Dream Plast India, a subsidiary of Gruppo Sunino, an Italian plastics maker with 10 plants around the world.\n",
    "\n",
    "\"The factory was first class,\" Kumar said. His contract included a monthly contribution from the company into a retirement fund and a bonus. Workers were served one meal every day, the supervisors were friendly, and the salary came on time, he said.\n",
    "\n",
    "Six days a week, his work typically involved overseeing two machines and a couple of contract workers. At the end of the day, he would relax with a game of badminton or watch wrestling on YouTube.\n",
    "\n",
    "His income over the past year helped his parents build a proper four-roomed brick home, after decades of living in a tumble-down mud hut where the roof let in heavy monsoon rains. It helped pay the fees for his brother to go to law school in Bahraich, an hour-and-a-half's drive away from their home village.\n",
    "\n",
    "Then COVID-19 struck.\n",
    "\n",
    "BROKE IN BARAMATI\n",
    "\n",
    "Kumar first heard of the coronavirus in early March. When India's lockdown forced Dream Plast India to temporarily shut its plant in Baramati on Mar 21, he had enough cash to wait it out in town.\n",
    "\n",
    "As the pandemic swept through India, a survey of some 5,000 workers in April and May found 66 per cent of participants had lost their jobs, and 77 per cent of households were consuming less food than before.\n",
    "\n",
    "Prime Minister Narendra Modi's government announced a 20 trillion rupee package promising free rice, wheat and pulses for millions of people and a programme to provide employment in rural areas.\n",
    "\n",
    "Even for those with work, trade unions and labour experts say conditions are deteriorating, for migrants particularly.  \n",
    "\n",
    "In May, India's state governments issued health and safety guidelines for factories as they re-opened after lockdown, which included compulsory face masks, thermal screening, social distancing and frequent sanitisation. Union leaders allege many companies did not follow all protocols and cut corners, but they have not identified Kumar's.\n",
    "\n",
    "Indian states including Uttar Pradesh and Gujarat said in May they were looking to relax workers' rights, including weakening regulations on wages and working hours, to support industry. That proposal drew criticism from workers' unions and the ILO. The amendments have only taken effect in some states.\n",
    "\n",
    "Kumar's factory, which reopened in early May, did not respond to a question on measures taken there, but Dream Plast India's managing director Nitin Gupta said in an email the \"company takes utmost precautions to adhere to the laws at all times\". He declined to elaborate further.\n",
    "\n",
    "Even so, Kumar and another worker Reuters spoke to said they did not feel safe to return.\n",
    "\n",
    "Ferrero, the Italian confectioner, said it had audited the plant where Kumar worked in March and found no irregularities, but would further review subsequent months.\n",
    "\n",
    "Reuters was unable to independently determine what safety measures the factory took.\n",
    "\n",
    "By early June, Kumar's funds had run out. Even buying food became difficult.\n",
    "\n",
    "His parents grew increasingly worried. \"Whatever little money I had here in the bank, I sent some of that so he could eat,\" said his father, Ashok. \"At that time, I was very scared. The biggest challenge was for him to come home.\"\n",
    "\n",
    "India's railway network reopened in early May. On Jun 3, Kumar borrowed money to pay for a 48-hour journey home by train, bus and shared taxi. Then he went into a 14-day quarantine.\n",
    "\n",
    "On Jun 25, Dream Plast India sent him an email, which was seen by Reuters, asking him to report to work within four days or face termination. Instead, he resigned on Jul 20.\n",
    "\n",
    "His parents are apprehensive about him leaving home again, although they said they realise that without their elder son's earnings, his younger brother will not be able to finish law school.\n",
    "\n",
    "Kumar is not ready to give up on his plastics factory.\n",
    "\n",
    "\"I will do it,\" he said. \"No matter what it takes, I will fulfil my dream.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from adapting https://huggingface.co/transformers/model_doc/t5.html\n",
    "import torch\n",
    "import json \n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "t5_prepared_Text = \"summarize: \"+preprocess_text\n",
    "print (\"original text preprocessed: \\n\", preprocess_text)\n",
    "\n",
    "tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "# summmarize \n",
    "summary_ids = model.generate(tokenized_text,\n",
    "                                    num_beams=4,\n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    min_length=30,\n",
    "                                    max_length=500,\n",
    "                                    early_stopping=True)\n",
    "\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print (\"\\n\\nSummarized text: \\n\",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Features in Development: Macro events"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
