{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7adfac8c",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02def98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5780</th>\n",
       "      <td>SINGAPORE - The Asia-Pacific unit of Swiss ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9837</th>\n",
       "      <td>KUALA LUMPUR (THE STAR/ASIA NEWS NETWORK) - Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12338</th>\n",
       "      <td>Elton John to end Vegas residencyBritish singe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12220</th>\n",
       "      <td>SINGAPORE - The Land Transport Authority (LTA)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10214</th>\n",
       "      <td>United States President Donald Trump and visit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>NEW YORK (BLOOMBERG) -Crude oil climbed along ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>RIO DE JANEIRO (NYTIMES) - Former vice-preside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13082</th>\n",
       "      <td>BAGHDAD (AFP) - Rocket and drone attacks on We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12971</th>\n",
       "      <td>THE HAGUE (AFP) - Two former Serbian spy chief...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9187</th>\n",
       "      <td>New filings for state benefits totalled 199,00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10980 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         content summary\n",
       "5780   SINGAPORE - The Asia-Pacific unit of Swiss ban...\n",
       "9837   KUALA LUMPUR (THE STAR/ASIA NEWS NETWORK) - Th...\n",
       "12338  Elton John to end Vegas residencyBritish singe...\n",
       "12220  SINGAPORE - The Land Transport Authority (LTA)...\n",
       "10214  United States President Donald Trump and visit...\n",
       "...                                                  ...\n",
       "1063   NEW YORK (BLOOMBERG) -Crude oil climbed along ...\n",
       "5908   RIO DE JANEIRO (NYTIMES) - Former vice-preside...\n",
       "13082  BAGHDAD (AFP) - Rocket and drone attacks on We...\n",
       "12971  THE HAGUE (AFP) - Two former Serbian spy chief...\n",
       "9187   New filings for state benefits totalled 199,00...\n",
       "\n",
       "[10980 rows x 1 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "cwd = os.path.abspath('../news Classifier/CSV Data for Classifier') \n",
    "files = os.listdir(cwd) \n",
    "df = pd.DataFrame()\n",
    "for file in files:\n",
    "    if file.endswith('.csv'):\n",
    "        df = df.append(pd.read_csv(cwd+\"/\"+file), ignore_index=True) \n",
    "        \n",
    "df = df.dropna()\n",
    "df.columns\n",
    "X = df.drop(['Unnamed: 0', 'date', 'location', 'news title', 'news source(url)',\n",
    "       'keywords', 'class_name', 'new_class_name'], axis=1)\n",
    "\n",
    "y = df['new_class_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2922ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the BaseEstimator\n",
    "import re \n",
    "from sklearn.base import BaseEstimator\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# define the class FeatureEngineering\n",
    "# This will be our custom transformer that will create 3 new binary columns\n",
    "# custom transformer must have methods fit and transform\n",
    "class FeatureEngineering(BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "    \n",
    "        \n",
    "    def transform(self, x_dataset):\n",
    "        \"\"\"\n",
    "        Function: split text into words and return the root form of the words\n",
    "        Args:\n",
    "          text(str): the article\n",
    "        Return:\n",
    "          lem(list of str): a list of the root form of the article words\n",
    "        \"\"\"\n",
    "        def preprocess(text):\n",
    "\n",
    "            # Normalize text\n",
    "            text = re.sub(r\"[^a-zA-Z]\", \" \", str(text).lower())\n",
    "\n",
    "            # Tokenize text\n",
    "            token = word_tokenize(text)\n",
    "\n",
    "            # Remove stop words\n",
    "            stop = stopwords.words(\"english\")\n",
    "            new_stop_words_list = ['said', 'us', 'also', 'mr']\n",
    "            stop.extend(new_stop_words_list)\n",
    "            words = [t for t in token if t not in stop]\n",
    "\n",
    "            # Lemmatization\n",
    "            lem = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "\n",
    "            return lem\n",
    "        \n",
    "        x_dataset.head()\n",
    "\n",
    "        x_dataset[\"Preprocessed_Text\"] = x_dataset['content summary'].apply(lambda x: preprocess(x))\n",
    "        x_dataset['Preprocessed_Text2'] = x_dataset['Preprocessed_Text'].apply(' '.join)\n",
    "        \n",
    "        \n",
    "        return x_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5583ef8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-467e96f9d59d>:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_dataset[\"Preprocessed_Text\"] = x_dataset['content summary'].apply(lambda x: preprocess(x))\n",
      "<ipython-input-2-467e96f9d59d>:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x_dataset['Preprocessed_Text2'] = x_dataset['Preprocessed_Text'].apply(' '.join)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "\n",
    "\n",
    "preprocessor = Pipeline(steps=[('feature engineering', FeatureEngineering()), \n",
    "                               ('col_selector', ColumnSelector(cols=('Preprocessed_Text2'),drop_axis=True)),\n",
    "                               ('tfidf',TfidfVectorizer()),\n",
    "                            ])\n",
    "\n",
    "\n",
    "train_features = preprocessor.fit(X_train)\n",
    "test_features = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "178258dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost \n",
    "from sklearn.metrics  import classification_report\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "def fit_eval_model(model, train_features, y_train, test_features, y_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function: train and evaluate a machine learning classifier.\n",
    "    Args:\n",
    "      model: machine learning classifier\n",
    "      train_features: train data extracted features\n",
    "      y_train: train data lables\n",
    "      test_features: train data extracted features\n",
    "      y_test: train data lables\n",
    "    Return:\n",
    "      results(dictionary): a dictionary of the model training time and classification report\n",
    "    \"\"\"\n",
    "    results ={}\n",
    "    \n",
    "    # Start time\n",
    "    start = time.time()\n",
    "    # Train the model\n",
    "    model.fit(train_features, y_train)\n",
    "    # End time\n",
    "    end = time.time()\n",
    "    # Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "    \n",
    "    # Test the model\n",
    "    train_predicted = model.predict(train_features)\n",
    "    test_predicted = model.predict(test_features)\n",
    "    \n",
    "     # Classification report\n",
    "    results['classification_report'] = classification_report(y_test, test_predicted)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46fe86b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b25ded2728eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mxgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'svm' is not defined"
     ]
    }
   ],
   "source": [
    "sv = svm.SVC()\n",
    "ab = AdaBoostClassifier(random_state = 1)\n",
    "gb = GradientBoostingClassifier(random_state = 1)\n",
    "xgb = xgboost.XGBClassifier(random_state = 1)\n",
    "tree = DecisionTreeClassifier()\n",
    "nb = MultinomialNB()\n",
    "\n",
    "\n",
    "# Fit and evaluate models\n",
    "results = {}\n",
    "for cls in [sv, ab, gb, xgb, tree, nb]:\n",
    "    cls_name = cls.__class__.__name__\n",
    "    results[cls_name] = {}\n",
    "    results[cls_name] = fit_eval_model(cls, train_features, y_train, test_features, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
